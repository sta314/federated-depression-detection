{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress = True)\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook runs for E-DAIC however DAIC-WOZ tests were not good either\n",
    "\n",
    "train_labels_df = pd.read_csv(\"edaicwoz/train_split.csv\")\n",
    "test_labels_df = pd.read_csv(\"edaicwoz/test_split.csv\")\n",
    "val_labels_df = pd.read_csv(\"edaicwoz/dev_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_files(data_dir, sr=16000):\n",
    "    file_ids = os.listdir(data_dir)\n",
    "    subject_ids = []\n",
    "    file_paths = []\n",
    "    types = []\n",
    "    labels = []\n",
    "    labels_binary = []\n",
    "\n",
    "    for file_id in file_ids:\n",
    "\n",
    "        file_id = file_id.split(\"_\")[0]\n",
    "        file_path = [data_dir + \"/\" + file_id + \"/\" + file_id + \"_MEL_\" + str(i) + \".npy\" for i in range(len(next(iter(enumerate(os.walk(data_dir + \"/\" + str(file_id) + \"/\"))))[1][2]))]\n",
    "        if int(file_id) in train_labels_df[\"Participant_ID\"].values:\n",
    "            types.append(0)\n",
    "            labels.append(train_labels_df[train_labels_df[\"Participant_ID\"] == int(file_id)]['PHQ_Score'].values[0])\n",
    "            labels_binary.append(train_labels_df[train_labels_df[\"Participant_ID\"] == int(file_id)]['PHQ_Binary'].values[0])\n",
    "            \n",
    "        elif int(file_id) in test_labels_df[\"Participant_ID\"].values:\n",
    "            types.append(1)\n",
    "            labels.append(test_labels_df[test_labels_df[\"Participant_ID\"] == int(file_id)]['PHQ_Score'].values[0])\n",
    "            labels_binary.append(test_labels_df[test_labels_df[\"Participant_ID\"] == int(file_id)]['PHQ_Binary'].values[0])\n",
    "        else:\n",
    "            types.append(2)\n",
    "            labels.append(val_labels_df[val_labels_df[\"Participant_ID\"] == int(file_id)]['PHQ_Score'].values[0])\n",
    "            labels_binary.append(val_labels_df[val_labels_df[\"Participant_ID\"] == int(file_id)]['PHQ_Binary'].values[0])\n",
    "        subject_ids.append(int(file_id))\n",
    "        file_paths.append(file_path)\n",
    "\n",
    "    return file_ids, subject_ids, file_paths, types, labels, labels_binary\n",
    "\n",
    "data_dir = \"MELs_40100_MM_SCA_CROP\"\n",
    "\n",
    "file_ids, subject_ids, file_paths, types, labels, labels_binary = load_audio_files(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] preparing data...\n"
     ]
    }
   ],
   "source": [
    "def prepare_audio_set(file_paths):\n",
    "\n",
    "    samples = []\n",
    "    samples_ids = []\n",
    "    samples_types = []\n",
    "    samples_labels = []\n",
    "    samples_labels_binary = []\n",
    "\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        all_mfccs = []\n",
    "        for j in range(len(file_path)):\n",
    "            all_mfccs.append(np.load(file_path[j]))\n",
    "        all_mfccs = np.array(all_mfccs)\n",
    "        samples.extend(all_mfccs)\n",
    "        samples_ids.extend([subject_ids[i]] * len(all_mfccs))\n",
    "        samples_types.extend([types[i]] * len(all_mfccs))\n",
    "        samples_labels.extend([labels[i]] * len(all_mfccs))\n",
    "        samples_labels_binary.extend([labels_binary[i]] * len(all_mfccs))\n",
    "\n",
    "    samples = np.array(samples)\n",
    "\n",
    "    samples_ids = np.array(samples_ids)\n",
    "    samples_types = np.array(samples_types)\n",
    "    samples_labels = np.array(samples_labels)\n",
    "    samples_labels_binary = np.array(samples_labels_binary)\n",
    "\n",
    "    return samples, samples_ids, samples_types, samples_labels, samples_labels_binary\n",
    "\n",
    "print(\"[INFO] preparing data...\")\n",
    "samples, samples_ids, samples_types, samples_labels, samples_labels_binary = prepare_audio_set(file_paths)\n",
    "samples = np.swapaxes(samples, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14920, 151, 80)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples = samples[samples_types == 0]\n",
    "training_labels = samples_labels_binary[samples_types == 0]\n",
    "\n",
    "test_samples = samples[samples_types == 1]\n",
    "test_labels = samples_labels_binary[samples_types == 1]\n",
    "\n",
    "val_samples = samples[samples_types == 2]\n",
    "val_labels = samples_labels_binary[samples_types == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] constructing training/testing split...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "    \n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size=16):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(self.x.shape[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_x = self.x[inds]\n",
    "        batch_y = self.y[inds]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "print(\"[INFO] constructing training/testing split...\")\n",
    "train_gen = DataGenerator(training_samples, training_labels, 4)\n",
    "test_gen = DataGenerator(test_samples, test_labels, 4)\n",
    "val_gen = DataGenerator(val_samples, val_labels, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14920, 151, 80)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 151, 80, 1)        0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 151, 80, 32)       256       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 148, 26, 32)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 148, 26, 32)       7200      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 148, 8, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 37888)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               4849792   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,873,889\n",
      "Trainable params: 4,873,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[INFO] fitting model...\n",
      "Epoch 1/30\n",
      "2370/2370 [==============================] - 17s 6ms/step - loss: 0.7036 - accuracy: 0.5326 - val_loss: 0.6896 - val_accuracy: 0.5500\n",
      "Epoch 2/30\n",
      "2370/2370 [==============================] - 14s 6ms/step - loss: 0.6826 - accuracy: 0.5564 - val_loss: 0.7020 - val_accuracy: 0.5394\n",
      "Epoch 3/30\n",
      "2370/2370 [==============================] - 14s 6ms/step - loss: 0.5794 - accuracy: 0.6836 - val_loss: 1.2636 - val_accuracy: 0.5503\n",
      "Epoch 4/30\n",
      "2370/2370 [==============================] - 15s 6ms/step - loss: 0.4663 - accuracy: 0.7759 - val_loss: 1.2169 - val_accuracy: 0.5459\n",
      "Epoch 5/30\n",
      "2370/2370 [==============================] - 15s 6ms/step - loss: 0.3623 - accuracy: 0.8406 - val_loss: 1.9650 - val_accuracy: 0.5222\n",
      "Epoch 6/30\n",
      "2370/2370 [==============================] - 15s 7ms/step - loss: 0.2798 - accuracy: 0.8905 - val_loss: 1.7814 - val_accuracy: 0.5369\n",
      "Epoch 7/30\n",
      "2370/2370 [==============================] - 15s 6ms/step - loss: 0.2215 - accuracy: 0.9218 - val_loss: 2.8744 - val_accuracy: 0.5537\n",
      "Epoch 8/30\n",
      "2370/2370 [==============================] - 16s 7ms/step - loss: 0.1872 - accuracy: 0.9365 - val_loss: 3.6149 - val_accuracy: 0.5094\n",
      "Epoch 9/30\n",
      "2370/2370 [==============================] - 15s 6ms/step - loss: 0.1584 - accuracy: 0.9524 - val_loss: 5.0956 - val_accuracy: 0.5131\n",
      "Epoch 10/30\n",
      "2370/2370 [==============================] - 14s 6ms/step - loss: 0.1288 - accuracy: 0.9621 - val_loss: 3.5812 - val_accuracy: 0.5216\n",
      "Epoch 11/30\n",
      "2370/2370 [==============================] - 14s 6ms/step - loss: 0.1170 - accuracy: 0.9659 - val_loss: 3.9915 - val_accuracy: 0.5653\n",
      "Epoch 12/30\n",
      "2370/2370 [==============================] - 15s 6ms/step - loss: 0.1101 - accuracy: 0.9722 - val_loss: 4.5949 - val_accuracy: 0.5347\n",
      "Epoch 13/30\n",
      "2370/2370 [==============================] - 14s 6ms/step - loss: 0.0908 - accuracy: 0.9775 - val_loss: 5.6026 - val_accuracy: 0.5278\n",
      "Epoch 14/30\n",
      "2370/2370 [==============================] - 15s 6ms/step - loss: 0.1077 - accuracy: 0.9754 - val_loss: 4.8980 - val_accuracy: 0.5750\n",
      "Epoch 15/30\n",
      "2370/2370 [==============================] - 15s 6ms/step - loss: 0.0873 - accuracy: 0.9775 - val_loss: 5.6843 - val_accuracy: 0.5384\n",
      "Epoch 16/30\n",
      "2370/2370 [==============================] - 14s 6ms/step - loss: 0.0880 - accuracy: 0.9782 - val_loss: 5.5108 - val_accuracy: 0.5491\n",
      "Epoch 17/30\n",
      "2370/2370 [==============================] - 15s 6ms/step - loss: 0.0694 - accuracy: 0.9826 - val_loss: 5.6510 - val_accuracy: 0.5556\n",
      "Epoch 18/30\n",
      "2370/2370 [==============================] - 14s 6ms/step - loss: 0.0767 - accuracy: 0.9841 - val_loss: 9.2200 - val_accuracy: 0.5069\n",
      "Epoch 19/30\n",
      "2370/2370 [==============================] - 15s 6ms/step - loss: 0.0761 - accuracy: 0.9821 - val_loss: 5.2682 - val_accuracy: 0.5225\n"
     ]
    }
   ],
   "source": [
    "from kapre.composed import get_melspectrogram_layer\n",
    "from kapre import LogmelToMFCC\n",
    "\n",
    "# Architecture details are specified in page 7 of paper\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(samples.shape[1], samples.shape[2])),\n",
    "    tf.keras.layers.Reshape((151, 80, 1)),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(1, 7), strides=(1, 1), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(4, 3), strides=(1, 3)),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(1, 7), strides=(1, 1), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(1, 3), strides=(1, 3)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "print(\"[INFO] compiling model...\")\n",
    "model.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=1), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, min_delta=0, restore_best_weights=True)\n",
    "\n",
    "print(\"[INFO] fitting model...\")\n",
    "history = model.fit(train_gen, epochs=30, validation_data=val_gen, callbacks=[early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
