{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r744gux56lPI",
        "outputId": "516cd8b5-11bb-42b6-8063-b98c4380c77c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting TensorFlow-privacy\n",
            "  Using cached tensorflow_privacy-0.8.12-py3-none-any.whl (405 kB)\n",
            "Requirement already satisfied: absl-py==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow-privacy) (1.4.0)\n",
            "Requirement already satisfied: attrs>=21.4 in /usr/local/lib/python3.10/dist-packages (from TensorFlow-privacy) (23.1.0)\n",
            "Requirement already satisfied: dm-tree==0.1.8 in /usr/local/lib/python3.10/dist-packages (from TensorFlow-privacy) (0.1.8)\n",
            "Requirement already satisfied: dp-accounting==0.4.3 in /usr/local/lib/python3.10/dist-packages (from TensorFlow-privacy) (0.4.3)\n",
            "Requirement already satisfied: immutabledict~=2.2 in /usr/local/lib/python3.10/dist-packages (from TensorFlow-privacy) (2.2.5)\n",
            "Requirement already satisfied: matplotlib~=3.3 in /usr/local/lib/python3.10/dist-packages (from TensorFlow-privacy) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.21 in /usr/local/lib/python3.10/dist-packages (from TensorFlow-privacy) (1.23.5)\n",
            "Requirement already satisfied: packaging~=22.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow-privacy) (22.0)\n",
            "Requirement already satisfied: pandas~=1.4 in /usr/local/lib/python3.10/dist-packages (from TensorFlow-privacy) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow-privacy) (1.2.2)\n",
            "Requirement already satisfied: scipy~=1.9 in /usr/local/lib/python3.10/dist-packages (from TensorFlow-privacy) (1.11.4)\n",
            "Requirement already satisfied: statsmodels~=0.13 in /usr/local/lib/python3.10/dist-packages (from TensorFlow-privacy) (0.14.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.4 in /usr/local/lib/python3.10/dist-packages (from TensorFlow-privacy) (2.15.0)\n",
            "Requirement already satisfied: tensorflow-probability~=0.22.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow-privacy) (0.22.0)\n",
            "Requirement already satisfied: tensorflow~=2.4 in /usr/local/lib/python3.10/dist-packages (from TensorFlow-privacy) (2.14.0)\n",
            "Collecting tf-models-official~=2.13 (from TensorFlow-privacy)\n",
            "  Using cached tf_models_official-2.15.0-py2.py3-none-any.whl (2.7 MB)\n",
            "Requirement already satisfied: mpmath~=1.2 in /usr/local/lib/python3.10/dist-packages (from dp-accounting==0.4.3->TensorFlow-privacy) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->TensorFlow-privacy) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->TensorFlow-privacy) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.3->TensorFlow-privacy) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.3->TensorFlow-privacy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.3->TensorFlow-privacy) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.3->TensorFlow-privacy) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.3->TensorFlow-privacy) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.3->TensorFlow-privacy) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.3->TensorFlow-privacy) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas~=1.4->TensorFlow-privacy) (2023.3.post1)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels~=0.13->TensorFlow-privacy) (0.5.4)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->TensorFlow-privacy) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->TensorFlow-privacy) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->TensorFlow-privacy) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->TensorFlow-privacy) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->TensorFlow-privacy) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->TensorFlow-privacy) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->TensorFlow-privacy) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->TensorFlow-privacy) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->TensorFlow-privacy) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->TensorFlow-privacy) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->TensorFlow-privacy) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->TensorFlow-privacy) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->TensorFlow-privacy) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->TensorFlow-privacy) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->TensorFlow-privacy) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->TensorFlow-privacy) (1.59.3)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow~=2.4->TensorFlow-privacy)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator~=2.4 (from TensorFlow-privacy)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.15,>=2.14.0 (from tensorflow~=2.4->TensorFlow-privacy)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->TensorFlow-privacy) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->TensorFlow-privacy) (2.2.1)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->TensorFlow-privacy) (3.0.6)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->TensorFlow-privacy) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->TensorFlow-privacy) (2.84.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->TensorFlow-privacy) (1.5.16)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->TensorFlow-privacy) (4.1.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->TensorFlow-privacy) (4.8.1.78)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->TensorFlow-privacy) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->TensorFlow-privacy) (9.0.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->TensorFlow-privacy) (2.0.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->TensorFlow-privacy) (6.0.1)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->TensorFlow-privacy) (2.4.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->TensorFlow-privacy) (0.1.99)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->TensorFlow-privacy) (1.2.2)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->TensorFlow-privacy) (4.9.3)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->TensorFlow-privacy) (0.15.0)\n",
            "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->TensorFlow-privacy) (0.7.5)\n",
            "Collecting tensorflow-text~=2.15.0 (from tf-models-official~=2.13->TensorFlow-privacy)\n",
            "  Using cached tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "Collecting tensorflow~=2.4 (from TensorFlow-privacy)\n",
            "  Using cached tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->TensorFlow-privacy) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->TensorFlow-privacy) (2.15.1)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->TensorFlow-privacy) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.4->TensorFlow-privacy) (0.42.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.13->TensorFlow-privacy) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.13->TensorFlow-privacy) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.13->TensorFlow-privacy) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.13->TensorFlow-privacy) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.13->TensorFlow-privacy) (4.1.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official~=2.13->TensorFlow-privacy) (2023.11.17)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official~=2.13->TensorFlow-privacy) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official~=2.13->TensorFlow-privacy) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official~=2.13->TensorFlow-privacy) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official~=2.13->TensorFlow-privacy) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official~=2.13->TensorFlow-privacy) (6.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.4->TensorFlow-privacy) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.4->TensorFlow-privacy) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.4->TensorFlow-privacy) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.4->TensorFlow-privacy) (3.0.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official~=2.13->TensorFlow-privacy) (0.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official~=2.13->TensorFlow-privacy) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official~=2.13->TensorFlow-privacy) (4.9)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official~=2.13->TensorFlow-privacy) (2.8.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official~=2.13->TensorFlow-privacy) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official~=2.13->TensorFlow-privacy) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official~=2.13->TensorFlow-privacy) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official~=2.13->TensorFlow-privacy) (4.9.3)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official~=2.13->TensorFlow-privacy) (0.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official~=2.13->TensorFlow-privacy) (8.1.7)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official~=2.13->TensorFlow-privacy) (1.5.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official~=2.13->TensorFlow-privacy) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official~=2.13->TensorFlow-privacy) (1.14.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official~=2.13->TensorFlow-privacy) (0.10.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official~=2.13->TensorFlow-privacy) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official~=2.13->TensorFlow-privacy) (6.1.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official~=2.13->TensorFlow-privacy) (3.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official~=2.13->TensorFlow-privacy) (1.61.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official~=2.13->TensorFlow-privacy) (5.3.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.4->TensorFlow-privacy) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle>=1.3.9->tf-models-official~=2.13->TensorFlow-privacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle>=1.3.9->tf-models-official~=2.13->TensorFlow-privacy) (3.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.4->TensorFlow-privacy) (2.1.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official~=2.13->TensorFlow-privacy) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official~=2.13->TensorFlow-privacy) (1.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.4->TensorFlow-privacy) (3.2.2)\n",
            "Installing collected packages: tensorflow, tensorflow-text, tf-models-official, TensorFlow-privacy\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.14.0\n",
            "    Uninstalling tensorflow-2.14.0:\n",
            "      Successfully uninstalled tensorflow-2.14.0\n",
            "Successfully installed TensorFlow-privacy-0.8.12 tensorflow-2.15.0.post1 tensorflow-text-2.15.0 tf-models-official-2.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q flwr[simulation]\n",
        "!pip install -U TensorFlow-privacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QhYdGQVU6py4"
      },
      "outputs": [],
      "source": [
        "import flwr as fl\n",
        "from flwr.common import Metrics\n",
        "from flwr.common.typing import NDArrays, Scalar\n",
        "from collections import OrderedDict\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUTPz2ww8yDy",
        "outputId": "af4a923f-444f-41fe-8919-fa2ac92a5d86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')#, force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if3RTu9E8zUl",
        "outputId": "049be6d5-1a27-4403-8162-b302fecfa6c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/daic/ProjectPrototype\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/MyDrive/daic/ProjectPrototype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJsOdA0d80eO",
        "outputId": "e2539799-0132-406c-f516-ea704cc63158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'212148conf copy 3.ipynb'   edaicwoz   MFCCs_1030   MFCCs_1030.zip   preprocess_data.ipynb\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NOdo724P811N"
      },
      "outputs": [],
      "source": [
        "train_labels_df = pd.read_csv(\"edaicwoz/train_split.csv\")\n",
        "test_labels_df = pd.read_csv(\"edaicwoz/test_split.csv\")\n",
        "val_labels_df = pd.read_csv(\"edaicwoz/dev_split.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4TEHAQf083qj"
      },
      "outputs": [],
      "source": [
        "def load_audio_files(data_dir, sr=16000):\n",
        "    file_ids = os.listdir(data_dir)\n",
        "    subject_ids = []\n",
        "    file_paths = []\n",
        "    types = []\n",
        "    labels = []\n",
        "    labels_binary = []\n",
        "\n",
        "    for file_id in file_ids:\n",
        "        file_id = file_id.split(\"_\")[0]\n",
        "        file_path = [data_dir + \"/\" + file_id + \"/\" + file_id + \"_MFCC_\" + str(i) + \".npy\" for i in range(len(next(iter(enumerate(os.walk(data_dir + \"/\" + str(file_id) + \"/\"))))[1][2]))]\n",
        "        if int(file_id) in train_labels_df[\"Participant_ID\"].values:\n",
        "            types.append(0)\n",
        "            labels.append(train_labels_df[train_labels_df[\"Participant_ID\"] == int(file_id)]['PHQ_Score'].values[0])\n",
        "            labels_binary.append(train_labels_df[train_labels_df[\"Participant_ID\"] == int(file_id)]['PHQ_Binary'].values[0])\n",
        "\n",
        "        elif int(file_id) in test_labels_df[\"Participant_ID\"].values:\n",
        "            types.append(1)\n",
        "            labels.append(test_labels_df[test_labels_df[\"Participant_ID\"] == int(file_id)]['PHQ_Score'].values[0])\n",
        "            labels_binary.append(test_labels_df[test_labels_df[\"Participant_ID\"] == int(file_id)]['PHQ_Binary'].values[0])\n",
        "        else:\n",
        "            types.append(2)\n",
        "            labels.append(val_labels_df[val_labels_df[\"Participant_ID\"] == int(file_id)]['PHQ_Score'].values[0])\n",
        "            labels_binary.append(val_labels_df[val_labels_df[\"Participant_ID\"] == int(file_id)]['PHQ_Binary'].values[0])\n",
        "        subject_ids.append(int(file_id))\n",
        "        file_paths.append(file_path)\n",
        "\n",
        "    return file_ids, subject_ids, file_paths, types, labels, labels_binary\n",
        "\n",
        "data_dir = \"MFCCs_1030\"\n",
        "\n",
        "file_ids, subject_ids, file_paths, types, labels, labels_binary = load_audio_files(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f54IRuEk889P",
        "outputId": "a3ed7290-8386-4036-9e1c-d0ae8028d0f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] preparing data...\n"
          ]
        }
      ],
      "source": [
        "def prepare_audio_set(file_paths):\n",
        "\n",
        "    samples = []\n",
        "    samples_ids = []\n",
        "    samples_types = []\n",
        "    samples_labels = []\n",
        "    samples_labels_binary = []\n",
        "\n",
        "    for i, file_path in enumerate(file_paths):\n",
        "        all_mfccs = []\n",
        "        for j in range(len(file_path)):\n",
        "            all_mfccs.append(np.load(file_path[j]))\n",
        "        all_mfccs = np.array(all_mfccs)\n",
        "        samples.extend(all_mfccs)\n",
        "        samples_ids.extend([subject_ids[i]] * len(all_mfccs))\n",
        "        samples_types.extend([types[i]] * len(all_mfccs))\n",
        "        samples_labels.extend([labels[i]] * len(all_mfccs))\n",
        "        samples_labels_binary.extend([labels_binary[i]] * len(all_mfccs))\n",
        "\n",
        "    samples = np.array(samples)\n",
        "\n",
        "    samples_ids = np.array(samples_ids)\n",
        "    samples_types = np.array(samples_types)\n",
        "    samples_labels = np.array(samples_labels)\n",
        "    samples_labels_binary = np.array(samples_labels_binary)\n",
        "\n",
        "    return samples, samples_ids, samples_types, samples_labels, samples_labels_binary\n",
        "\n",
        "print(\"[INFO] preparing data...\")\n",
        "samples, samples_ids, samples_types, samples_labels, samples_labels_binary = prepare_audio_set(file_paths)\n",
        "samples = np.swapaxes(samples, 1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uMLrvS1L9F8v"
      },
      "outputs": [],
      "source": [
        "training_samples = samples[samples_types == 0]\n",
        "training_labels = samples_labels_binary[samples_types == 0]\n",
        "training_subject_ids = samples_ids[samples_types == 0]\n",
        "\n",
        "test_samples = samples[samples_types == 1]\n",
        "test_labels = samples_labels_binary[samples_types == 1]\n",
        "\n",
        "val_samples = samples[samples_types == 2]\n",
        "val_labels_df = samples_labels_binary[samples_types == 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bpe1dRB-8WdZ"
      },
      "outputs": [],
      "source": [
        "del samples, samples_ids, samples_types, samples_labels, samples_labels_binary\n",
        "del file_ids, subject_ids, file_paths, types, labels, labels_binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PDSzvY3L6woq"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "NUM_CLIENTS = 4\n",
        "BATCH_SIZE = 32\n",
        "NUM_ROUNDS = 5\n",
        "DEPRESSIVE_MULTIPLIER = 30\n",
        "NON_DEPRESSIVE_MULTIPLIER = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoLCYLk-60oa",
        "outputId": "23809924-fa6f-457a-fb37-05d9aaa55f5a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-45e26c792d34>:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  training_samples_grouped = np.array(training_samples_grouped)\n"
          ]
        }
      ],
      "source": [
        "def partition_data(X: np.ndarray, X_ids: np.ndarray, n_clients: int, d_mult: int, n_d_mult: int) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
        "\n",
        "    unique_subject_ids, counts = np.unique(X_ids, return_counts=True)\n",
        "\n",
        "    # grouping the training samples by patient\n",
        "    training_samples_grouped = []\n",
        "    for i in unique_subject_ids:\n",
        "        training_samples_grouped.append(X[X_ids == i])\n",
        "    training_samples_grouped = np.array(training_samples_grouped)\n",
        "\n",
        "    mask_30_segments = np.array([sample.shape[0] == DEPRESSIVE_MULTIPLIER for sample in training_samples_grouped])\n",
        "\n",
        "    # creating masks to get deppressives and non deppressives\n",
        "    mask_10_segments = np.array([sample.shape[0] == NON_DEPRESSIVE_MULTIPLIER for sample in training_samples_grouped])\n",
        "\n",
        "    data_array_30_segments = training_samples_grouped[mask_30_segments]\n",
        "\n",
        "    data_array_10_segments = training_samples_grouped[mask_10_segments]\n",
        "\n",
        "    # recreating labels\n",
        "    X_train_zeros = np.array_split(data_array_10_segments, NUM_CLIENTS)\n",
        "    X_train_ones = np.array_split(data_array_30_segments, NUM_CLIENTS)\n",
        "\n",
        "    # concatenating splitted ones and zeros with labels\n",
        "    X_train_splitted = [] # (NUM_CLIENTS, data)\n",
        "    y_train_splitted = [] # (NUM_CLIENTS, labels)\n",
        "    for i in range(NUM_CLIENTS):\n",
        "\n",
        "        # stack the segments from groups then concatenate\n",
        "        client_data = np.concatenate((np.vstack(X_train_zeros[i]), np.vstack(X_train_ones[i])), axis=0)\n",
        "        client_labels = np.concatenate((np.zeros((X_train_zeros[i].shape[0] * NON_DEPRESSIVE_MULTIPLIER), dtype=int), np.ones((X_train_ones[i].shape[0] * DEPRESSIVE_MULTIPLIER), dtype=int)), axis=0)\n",
        "\n",
        "        X_train_splitted.append(client_data)\n",
        "        y_train_splitted.append(client_labels)\n",
        "\n",
        "    return X_train_splitted, y_train_splitted\n",
        "\n",
        "X_trains, y_trains = partition_data(training_samples, training_subject_ids, NUM_CLIENTS, DEPRESSIVE_MULTIPLIER, NON_DEPRESSIVE_MULTIPLIER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOakTnvyGeG9",
        "outputId": "0536affa-2519-405d-c4ee-e825b90ea146"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(610, 15001, 13)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_trains[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6TLISBYkEXxm"
      },
      "outputs": [],
      "source": [
        "def get_model(input_shape):\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(tf.keras.layers.GRU(units = 64, input_shape = input_shape)) # , return_sequences=True\n",
        "    # model.add(tf.keras.layers.GRU(units = 32))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    #model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1), metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NqrDDSKP7HtP"
      },
      "outputs": [],
      "source": [
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras_vectorized import (\n",
        "    VectorizedDPKerasSGDOptimizer,\n",
        ")\n",
        "from flwr.common.typing import NDArrays\n",
        "class FlowerClient(fl.client.NumPyClient):\n",
        "\n",
        "    def __init__(self, model: tf.keras.models.Sequential, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        self.model = model\n",
        "\n",
        "        # Dropping uneven batches\n",
        "        if X_train.shape[0] % BATCH_SIZE != 0:\n",
        "            drop_num = X_train.shape[0] % BATCH_SIZE\n",
        "            X_train = X_train[:-drop_num]\n",
        "            y_train = y_train[:-drop_num]\n",
        "\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return self.model.get_weights()\n",
        "\n",
        "\n",
        "    def fit(self, parameters: NDArrays, config: Dict[str, Scalar]) -> NDArrays:\n",
        "\n",
        "        optimizer = VectorizedDPKerasSGDOptimizer(l2_norm_clip=1.0, noise_multiplier=1.1, num_microbatches=8, learning_rate=0.0005)\n",
        "        loss = tf.keras.losses.BinaryCrossentropy( from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "\n",
        "        self.model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
        "        self.model.set_weights(parameters)\n",
        "\n",
        "        history = self.model.fit(self.X_train, self.y_train ,batch_size=BATCH_SIZE, epochs=1, verbose=0)\n",
        "        results = {\n",
        "            \"loss\": history.history[\"loss\"][0],\n",
        "            \"accuracy\": history.history[\"accuracy\"][0],\n",
        "        }\n",
        "        return self.model.get_weights(), len(self.X_train), results\n",
        "\n",
        "    def evaluate(self, parameters: NDArrays, config: Dict[str, Scalar])-> Tuple[float, int, Dict[str, Scalar]]:\n",
        "        optimizer = VectorizedDPKerasSGDOptimizer(l2_norm_clip=1.0, noise_multiplier=1.1, num_microbatches=8, learning_rate=0.0005)\n",
        "        loss = tf.keras.losses.BinaryCrossentropy( from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "\n",
        "        self.model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
        "        self.model.set_weights(parameters)\n",
        "\n",
        "        loss, acc = self.model.evaluate(self.X_train, self.y_train, verbose=0)\n",
        "        return loss, len(self.X_train), {\"accuracy\": acc}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qgf9oz_57I9I"
      },
      "outputs": [],
      "source": [
        "# client creator by client id\n",
        "def create_client_fn(cid: str) -> FlowerClient:\n",
        "\n",
        "    input_shape = (15001, 13)\n",
        "    model = get_model(input_shape)\n",
        "    cid_int = int(cid)\n",
        "    return FlowerClient(model, X_trains[cid_int], y_trains[cid_int])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-BT6BuAA7J3j"
      },
      "outputs": [],
      "source": [
        "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
        "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "\n",
        "    # Aggregate and return custom metric (weighted average)\n",
        "    return {\"accuracy\": sum(accuracies) / sum(examples)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l4S_aJJ7Laq",
        "outputId": "a29579bc-433c-4a0d-d880-c8d72cb6b654"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO flwr 2023-12-14 05:11:48,180 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=15, round_timeout=None)\n",
            "INFO:flwr:Starting Flower simulation, config: ServerConfig(num_rounds=15, round_timeout=None)\n",
            "2023-12-14 05:11:50,475\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "INFO flwr 2023-12-14 05:11:52,021 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 8.0, 'memory': 32515716711.0, 'node:172.28.0.12': 1.0, 'GPU': 1.0, 'object_store_memory': 16257858355.0, 'node:__internal_head__': 1.0}\n",
            "INFO:flwr:Flower VCE: Ray initialized with resources: {'CPU': 8.0, 'memory': 32515716711.0, 'node:172.28.0.12': 1.0, 'GPU': 1.0, 'object_store_memory': 16257858355.0, 'node:__internal_head__': 1.0}\n",
            "INFO flwr 2023-12-14 05:11:52,024 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
            "INFO:flwr:Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
            "INFO flwr 2023-12-14 05:11:52,027 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2}\n",
            "INFO:flwr:Flower VCE: Resources for each Virtual Client: {'num_cpus': 2}\n",
            "INFO flwr 2023-12-14 05:11:52,045 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 4 actors\n",
            "INFO:flwr:Flower VCE: Creating VirtualClientEngineActorPool with 4 actors\n",
            "INFO flwr 2023-12-14 05:11:52,047 | server.py:89 | Initializing global parameters\n",
            "INFO:flwr:Initializing global parameters\n",
            "INFO flwr 2023-12-14 05:11:52,051 | server.py:276 | Requesting initial parameters from one random client\n",
            "INFO:flwr:Requesting initial parameters from one random client\n",
            "\u001b[2m\u001b[36m(pid=10609)\u001b[0m 2023-12-14 05:11:53.367342: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=10609)\u001b[0m 2023-12-14 05:11:53.367399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=10609)\u001b[0m 2023-12-14 05:11:53.368892: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=10611)\u001b[0m 2023-12-14 05:11:54.759266: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2m\u001b[36m(pid=10611)\u001b[0m 2023-12-14 05:11:53.584162: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[2m\u001b[36m(pid=10611)\u001b[0m 2023-12-14 05:11:53.584208: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(pid=10611)\u001b[0m 2023-12-14 05:11:53.585410: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=10611)\u001b[0m 2023-12-14 05:12:02.716614: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[2m\u001b[36m(pid=10610)\u001b[0m 2023-12-14 05:11:55.030734: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "INFO flwr 2023-12-14 05:12:03,207 | server.py:280 | Received initial parameters from one random client\n",
            "INFO:flwr:Received initial parameters from one random client\n",
            "INFO flwr 2023-12-14 05:12:03,209 | server.py:91 | Evaluating initial parameters\n",
            "INFO:flwr:Evaluating initial parameters\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n",
            "INFO flwr 2023-12-14 05:12:44,605 | server.py:94 | initial parameters (loss, other metrics): 0.7101017236709595, {'accuracy': 0.5224999785423279}\n",
            "INFO:flwr:initial parameters (loss, other metrics): 0.7101017236709595, {'accuracy': 0.5224999785423279}\n",
            "INFO flwr 2023-12-14 05:12:44,608 | server.py:104 | FL starting\n",
            "INFO:flwr:FL starting\n",
            "DEBUG flwr 2023-12-14 05:12:44,612 | server.py:222 | fit_round 1: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:fit_round 1: strategy sampled 4 clients (out of 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS: 0.7101017236709595\n",
            "BEST_LOSS: 999\n",
            "ACCURACY: 0.5224999785423279\n",
            "BEST_ACCURACY: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=10611)\u001b[0m /usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=10611)\u001b[0m   output, from_logits = _get_logits(\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=10610)\u001b[0m 2023-12-14 05:13:01.554553: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=10610)\u001b[0m /usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=10610)\u001b[0m   output, from_logits = _get_logits(\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=10609)\u001b[0m 2023-12-14 05:13:11.396729: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=10609)\u001b[0m /usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=10609)\u001b[0m   output, from_logits = _get_logits(\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=10607)\u001b[0m 2023-12-14 05:13:24.259082: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=10607)\u001b[0m /usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=10607)\u001b[0m   output, from_logits = _get_logits(\n",
            "DEBUG flwr 2023-12-14 05:19:14,555 | server.py:236 | fit_round 1 received 4 results and 0 failures\n",
            "DEBUG:flwr:fit_round 1 received 4 results and 0 failures\n",
            "WARNING flwr 2023-12-14 05:19:14,564 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
            "WARNING:flwr:No fit_metrics_aggregation_fn provided\n",
            "INFO flwr 2023-12-14 05:19:56,568 | server.py:125 | fit progress: (1, 0.709667980670929, {'accuracy': 0.5212500095367432}, 431.9559547150002)\n",
            "INFO:flwr:fit progress: (1, 0.709667980670929, {'accuracy': 0.5212500095367432}, 431.9559547150002)\n",
            "DEBUG flwr 2023-12-14 05:19:56,570 | server.py:173 | evaluate_round 1: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:evaluate_round 1: strategy sampled 4 clients (out of 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS: 0.709667980670929\n",
            "BEST_LOSS: 0.7101017236709595\n",
            "ACCURACY: 0.5212500095367432\n",
            "BEST_ACCURACY: 0.5224999785423279\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-12-14 05:21:05,973 | server.py:187 | evaluate_round 1 received 4 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 1 received 4 results and 0 failures\n",
            "DEBUG flwr 2023-12-14 05:21:05,976 | server.py:222 | fit_round 2: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:fit_round 2: strategy sampled 4 clients (out of 4)\n",
            "DEBUG flwr 2023-12-14 05:27:34,758 | server.py:236 | fit_round 2 received 4 results and 0 failures\n",
            "DEBUG:flwr:fit_round 2 received 4 results and 0 failures\n",
            "INFO flwr 2023-12-14 05:28:16,947 | server.py:125 | fit progress: (2, 0.7094146013259888, {'accuracy': 0.5162500143051147}, 932.3350544099999)\n",
            "INFO:flwr:fit progress: (2, 0.7094146013259888, {'accuracy': 0.5162500143051147}, 932.3350544099999)\n",
            "DEBUG flwr 2023-12-14 05:28:16,950 | server.py:173 | evaluate_round 2: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:evaluate_round 2: strategy sampled 4 clients (out of 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS: 0.7094146013259888\n",
            "BEST_LOSS: 0.709667980670929\n",
            "ACCURACY: 0.5162500143051147\n",
            "BEST_ACCURACY: 0.5212500095367432\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-12-14 05:29:21,840 | server.py:187 | evaluate_round 2 received 4 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 2 received 4 results and 0 failures\n",
            "DEBUG flwr 2023-12-14 05:29:21,843 | server.py:222 | fit_round 3: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:fit_round 3: strategy sampled 4 clients (out of 4)\n",
            "DEBUG flwr 2023-12-14 05:35:48,897 | server.py:236 | fit_round 3 received 4 results and 0 failures\n",
            "DEBUG:flwr:fit_round 3 received 4 results and 0 failures\n",
            "INFO flwr 2023-12-14 05:36:30,233 | server.py:125 | fit progress: (3, 0.7095325589179993, {'accuracy': 0.5162500143051147}, 1425.621207226)\n",
            "INFO:flwr:fit progress: (3, 0.7095325589179993, {'accuracy': 0.5162500143051147}, 1425.621207226)\n",
            "DEBUG flwr 2023-12-14 05:36:30,239 | server.py:173 | evaluate_round 3: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:evaluate_round 3: strategy sampled 4 clients (out of 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS: 0.7095325589179993\n",
            "BEST_LOSS: 0.7094146013259888\n",
            "ACCURACY: 0.5162500143051147\n",
            "BEST_ACCURACY: 0.5162500143051147\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-12-14 05:37:35,670 | server.py:187 | evaluate_round 3 received 4 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 3 received 4 results and 0 failures\n",
            "DEBUG flwr 2023-12-14 05:37:35,673 | server.py:222 | fit_round 4: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:fit_round 4: strategy sampled 4 clients (out of 4)\n",
            "DEBUG flwr 2023-12-14 05:44:06,179 | server.py:236 | fit_round 4 received 4 results and 0 failures\n",
            "DEBUG:flwr:fit_round 4 received 4 results and 0 failures\n",
            "INFO flwr 2023-12-14 05:44:46,517 | server.py:125 | fit progress: (4, 0.7093674540519714, {'accuracy': 0.5149999856948853}, 1921.9053288670002)\n",
            "INFO:flwr:fit progress: (4, 0.7093674540519714, {'accuracy': 0.5149999856948853}, 1921.9053288670002)\n",
            "DEBUG flwr 2023-12-14 05:44:46,520 | server.py:173 | evaluate_round 4: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:evaluate_round 4: strategy sampled 4 clients (out of 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS: 0.7093674540519714\n",
            "BEST_LOSS: 0.7094146013259888\n",
            "ACCURACY: 0.5149999856948853\n",
            "BEST_ACCURACY: 0.5162500143051147\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-12-14 05:45:49,902 | server.py:187 | evaluate_round 4 received 4 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 4 received 4 results and 0 failures\n",
            "DEBUG flwr 2023-12-14 05:45:49,904 | server.py:222 | fit_round 5: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:fit_round 5: strategy sampled 4 clients (out of 4)\n",
            "DEBUG flwr 2023-12-14 05:52:07,854 | server.py:236 | fit_round 5 received 4 results and 0 failures\n",
            "DEBUG:flwr:fit_round 5 received 4 results and 0 failures\n",
            "INFO flwr 2023-12-14 05:52:47,510 | server.py:125 | fit progress: (5, 0.7088690400123596, {'accuracy': 0.518750011920929}, 2402.89787618)\n",
            "INFO:flwr:fit progress: (5, 0.7088690400123596, {'accuracy': 0.518750011920929}, 2402.89787618)\n",
            "DEBUG flwr 2023-12-14 05:52:47,514 | server.py:173 | evaluate_round 5: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:evaluate_round 5: strategy sampled 4 clients (out of 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS: 0.7088690400123596\n",
            "BEST_LOSS: 0.7093674540519714\n",
            "ACCURACY: 0.518750011920929\n",
            "BEST_ACCURACY: 0.5149999856948853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR flwr 2023-12-14 05:53:21,932 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 05:53:21,936 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 05:53:47,702 | server.py:187 | evaluate_round 5 received 3 results and 1 failures\n",
            "DEBUG:flwr:evaluate_round 5 received 3 results and 1 failures\n",
            "DEBUG flwr 2023-12-14 05:53:47,705 | server.py:222 | fit_round 6: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:fit_round 6: strategy sampled 4 clients (out of 4)\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-12-14 05:53:50,522 E 10372 10372] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0, IP: 172.28.0.12) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.28.0.12`\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2023-12-14 05:54:19,823 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 05:54:19,827 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 05:58:44,089 | server.py:236 | fit_round 6 received 3 results and 1 failures\n",
            "DEBUG:flwr:fit_round 6 received 3 results and 1 failures\n",
            "INFO flwr 2023-12-14 05:59:24,654 | server.py:125 | fit progress: (6, 0.7085859179496765, {'accuracy': 0.5149999856948853}, 2800.0415601619998)\n",
            "INFO:flwr:fit progress: (6, 0.7085859179496765, {'accuracy': 0.5149999856948853}, 2800.0415601619998)\n",
            "DEBUG flwr 2023-12-14 05:59:24,656 | server.py:173 | evaluate_round 6: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:evaluate_round 6: strategy sampled 4 clients (out of 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS: 0.7085859179496765\n",
            "BEST_LOSS: 0.7088690400123596\n",
            "ACCURACY: 0.5149999856948853\n",
            "BEST_ACCURACY: 0.518750011920929\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR flwr 2023-12-14 05:59:59,427 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 05:59:59,432 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:00:18,104 | server.py:187 | evaluate_round 6 received 3 results and 1 failures\n",
            "DEBUG:flwr:evaluate_round 6 received 3 results and 1 failures\n",
            "DEBUG flwr 2023-12-14 06:00:18,107 | server.py:222 | fit_round 7: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:fit_round 7: strategy sampled 4 clients (out of 4)\n",
            "ERROR flwr 2023-12-14 06:00:49,560 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 06:00:49,564 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:05:21,596 | server.py:236 | fit_round 7 received 3 results and 1 failures\n",
            "DEBUG:flwr:fit_round 7 received 3 results and 1 failures\n",
            "INFO flwr 2023-12-14 06:06:02,689 | server.py:125 | fit progress: (7, 0.7081944346427917, {'accuracy': 0.5199999809265137}, 3198.0772588809996)\n",
            "INFO:flwr:fit progress: (7, 0.7081944346427917, {'accuracy': 0.5199999809265137}, 3198.0772588809996)\n",
            "DEBUG flwr 2023-12-14 06:06:02,692 | server.py:173 | evaluate_round 7: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:evaluate_round 7: strategy sampled 4 clients (out of 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS: 0.7081944346427917\n",
            "BEST_LOSS: 0.7085859179496765\n",
            "ACCURACY: 0.5199999809265137\n",
            "BEST_ACCURACY: 0.5149999856948853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR flwr 2023-12-14 06:06:36,702 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 06:06:36,710 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:06:50,972 | server.py:187 | evaluate_round 7 received 3 results and 1 failures\n",
            "DEBUG:flwr:evaluate_round 7 received 3 results and 1 failures\n",
            "DEBUG flwr 2023-12-14 06:06:50,975 | server.py:222 | fit_round 8: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:fit_round 8: strategy sampled 4 clients (out of 4)\n",
            "ERROR flwr 2023-12-14 06:07:24,030 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 06:07:24,048 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:11:47,747 | server.py:236 | fit_round 8 received 3 results and 1 failures\n",
            "DEBUG:flwr:fit_round 8 received 3 results and 1 failures\n",
            "INFO flwr 2023-12-14 06:12:28,811 | server.py:125 | fit progress: (8, 0.7079393863677979, {'accuracy': 0.5212500095367432}, 3584.1983748209996)\n",
            "INFO:flwr:fit progress: (8, 0.7079393863677979, {'accuracy': 0.5212500095367432}, 3584.1983748209996)\n",
            "DEBUG flwr 2023-12-14 06:12:28,814 | server.py:173 | evaluate_round 8: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:evaluate_round 8: strategy sampled 4 clients (out of 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS: 0.7079393863677979\n",
            "BEST_LOSS: 0.7081944346427917\n",
            "ACCURACY: 0.5212500095367432\n",
            "BEST_ACCURACY: 0.5199999809265137\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR flwr 2023-12-14 06:13:10,970 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 06:13:10,975 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:13:20,151 | server.py:187 | evaluate_round 8 received 3 results and 1 failures\n",
            "DEBUG:flwr:evaluate_round 8 received 3 results and 1 failures\n",
            "DEBUG flwr 2023-12-14 06:13:20,155 | server.py:222 | fit_round 9: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:fit_round 9: strategy sampled 4 clients (out of 4)\n",
            "ERROR flwr 2023-12-14 06:13:52,947 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 06:13:52,953 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:18:17,425 | server.py:236 | fit_round 9 received 3 results and 1 failures\n",
            "DEBUG:flwr:fit_round 9 received 3 results and 1 failures\n",
            "INFO flwr 2023-12-14 06:18:58,633 | server.py:125 | fit progress: (9, 0.7072866559028625, {'accuracy': 0.518750011920929}, 3974.020372473)\n",
            "INFO:flwr:fit progress: (9, 0.7072866559028625, {'accuracy': 0.518750011920929}, 3974.020372473)\n",
            "DEBUG flwr 2023-12-14 06:18:58,635 | server.py:173 | evaluate_round 9: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:evaluate_round 9: strategy sampled 4 clients (out of 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS: 0.7072866559028625\n",
            "BEST_LOSS: 0.7079393863677979\n",
            "ACCURACY: 0.518750011920929\n",
            "BEST_ACCURACY: 0.5212500095367432\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR flwr 2023-12-14 06:19:34,127 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 06:19:34,132 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:19:56,436 | server.py:187 | evaluate_round 9 received 3 results and 1 failures\n",
            "DEBUG:flwr:evaluate_round 9 received 3 results and 1 failures\n",
            "DEBUG flwr 2023-12-14 06:19:56,443 | server.py:222 | fit_round 10: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:fit_round 10: strategy sampled 4 clients (out of 4)\n",
            "ERROR flwr 2023-12-14 06:20:28,259 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 06:20:28,266 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:24:57,470 | server.py:236 | fit_round 10 received 3 results and 1 failures\n",
            "DEBUG:flwr:fit_round 10 received 3 results and 1 failures\n",
            "INFO flwr 2023-12-14 06:25:40,013 | server.py:125 | fit progress: (10, 0.7064116597175598, {'accuracy': 0.5199999809265137}, 4375.401284326)\n",
            "INFO:flwr:fit progress: (10, 0.7064116597175598, {'accuracy': 0.5199999809265137}, 4375.401284326)\n",
            "DEBUG flwr 2023-12-14 06:25:40,016 | server.py:173 | evaluate_round 10: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:evaluate_round 10: strategy sampled 4 clients (out of 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS: 0.7064116597175598\n",
            "BEST_LOSS: 0.7072866559028625\n",
            "ACCURACY: 0.5199999809265137\n",
            "BEST_ACCURACY: 0.518750011920929\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR flwr 2023-12-14 06:26:15,933 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 06:26:15,938 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:26:34,235 | server.py:187 | evaluate_round 10 received 3 results and 1 failures\n",
            "DEBUG:flwr:evaluate_round 10 received 3 results and 1 failures\n",
            "DEBUG flwr 2023-12-14 06:26:34,241 | server.py:222 | fit_round 11: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:fit_round 11: strategy sampled 4 clients (out of 4)\n",
            "ERROR flwr 2023-12-14 06:27:06,888 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 06:27:06,894 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:31:32,718 | server.py:236 | fit_round 11 received 3 results and 1 failures\n",
            "DEBUG:flwr:fit_round 11 received 3 results and 1 failures\n",
            "INFO flwr 2023-12-14 06:32:13,961 | server.py:125 | fit progress: (11, 0.7057126760482788, {'accuracy': 0.5237500071525574}, 4769.348444405)\n",
            "INFO:flwr:fit progress: (11, 0.7057126760482788, {'accuracy': 0.5237500071525574}, 4769.348444405)\n",
            "DEBUG flwr 2023-12-14 06:32:13,965 | server.py:173 | evaluate_round 11: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:evaluate_round 11: strategy sampled 4 clients (out of 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS: 0.7057126760482788\n",
            "BEST_LOSS: 0.7064116597175598\n",
            "ACCURACY: 0.5237500071525574\n",
            "BEST_ACCURACY: 0.5199999809265137\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR flwr 2023-12-14 06:32:47,946 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 06:32:47,950 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:33:05,058 | server.py:187 | evaluate_round 11 received 3 results and 1 failures\n",
            "DEBUG:flwr:evaluate_round 11 received 3 results and 1 failures\n",
            "DEBUG flwr 2023-12-14 06:33:05,060 | server.py:222 | fit_round 12: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:fit_round 12: strategy sampled 4 clients (out of 4)\n",
            "ERROR flwr 2023-12-14 06:33:37,933 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 06:33:37,943 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:38:03,137 | server.py:236 | fit_round 12 received 3 results and 1 failures\n",
            "DEBUG:flwr:fit_round 12 received 3 results and 1 failures\n",
            "INFO flwr 2023-12-14 06:38:45,064 | server.py:125 | fit progress: (12, 0.7047927379608154, {'accuracy': 0.5249999761581421}, 5160.451381319)\n",
            "INFO:flwr:fit progress: (12, 0.7047927379608154, {'accuracy': 0.5249999761581421}, 5160.451381319)\n",
            "DEBUG flwr 2023-12-14 06:38:45,066 | server.py:173 | evaluate_round 12: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:evaluate_round 12: strategy sampled 4 clients (out of 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS: 0.7047927379608154\n",
            "BEST_LOSS: 0.7057126760482788\n",
            "ACCURACY: 0.5249999761581421\n",
            "BEST_ACCURACY: 0.5237500071525574\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR flwr 2023-12-14 06:39:21,057 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 06:39:21,069 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:39:36,765 | server.py:187 | evaluate_round 12 received 3 results and 1 failures\n",
            "DEBUG:flwr:evaluate_round 12 received 3 results and 1 failures\n",
            "DEBUG flwr 2023-12-14 06:39:36,771 | server.py:222 | fit_round 13: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:fit_round 13: strategy sampled 4 clients (out of 4)\n",
            "ERROR flwr 2023-12-14 06:40:09,452 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 06:40:09,459 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:44:29,393 | server.py:236 | fit_round 13 received 3 results and 1 failures\n",
            "DEBUG:flwr:fit_round 13 received 3 results and 1 failures\n",
            "INFO flwr 2023-12-14 06:45:09,769 | server.py:125 | fit progress: (13, 0.7041099667549133, {'accuracy': 0.5249999761581421}, 5545.15702039)\n",
            "INFO:flwr:fit progress: (13, 0.7041099667549133, {'accuracy': 0.5249999761581421}, 5545.15702039)\n",
            "DEBUG flwr 2023-12-14 06:45:09,771 | server.py:173 | evaluate_round 13: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:evaluate_round 13: strategy sampled 4 clients (out of 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS: 0.7041099667549133\n",
            "BEST_LOSS: 0.7047927379608154\n",
            "ACCURACY: 0.5249999761581421\n",
            "BEST_ACCURACY: 0.5249999761581421\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR flwr 2023-12-14 06:45:52,915 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 06:45:52,919 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:46:09,767 | server.py:187 | evaluate_round 13 received 3 results and 1 failures\n",
            "DEBUG:flwr:evaluate_round 13 received 3 results and 1 failures\n",
            "DEBUG flwr 2023-12-14 06:46:09,770 | server.py:222 | fit_round 14: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:fit_round 14: strategy sampled 4 clients (out of 4)\n",
            "ERROR flwr 2023-12-14 06:46:42,337 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 06:46:42,350 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:51:06,302 | server.py:236 | fit_round 14 received 3 results and 1 failures\n",
            "DEBUG:flwr:fit_round 14 received 3 results and 1 failures\n",
            "INFO flwr 2023-12-14 06:51:47,575 | server.py:125 | fit progress: (14, 0.7028810977935791, {'accuracy': 0.5237500071525574}, 5942.963265369999)\n",
            "INFO:flwr:fit progress: (14, 0.7028810977935791, {'accuracy': 0.5237500071525574}, 5942.963265369999)\n",
            "DEBUG flwr 2023-12-14 06:51:47,578 | server.py:173 | evaluate_round 14: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:evaluate_round 14: strategy sampled 4 clients (out of 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS: 0.7028810977935791\n",
            "BEST_LOSS: 0.7041099667549133\n",
            "ACCURACY: 0.5237500071525574\n",
            "BEST_ACCURACY: 0.5249999761581421\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR flwr 2023-12-14 06:52:22,431 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 06:52:22,436 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:52:40,241 | server.py:187 | evaluate_round 14 received 3 results and 1 failures\n",
            "DEBUG:flwr:evaluate_round 14 received 3 results and 1 failures\n",
            "DEBUG flwr 2023-12-14 06:52:40,243 | server.py:222 | fit_round 15: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:fit_round 15: strategy sampled 4 clients (out of 4)\n",
            "ERROR flwr 2023-12-14 06:53:12,220 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 06:53:12,225 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:57:35,508 | server.py:236 | fit_round 15 received 3 results and 1 failures\n",
            "DEBUG:flwr:fit_round 15 received 3 results and 1 failures\n",
            "INFO flwr 2023-12-14 06:58:17,796 | server.py:125 | fit progress: (15, 0.7032623887062073, {'accuracy': 0.5249999761581421}, 6333.184068160001)\n",
            "INFO:flwr:fit progress: (15, 0.7032623887062073, {'accuracy': 0.5249999761581421}, 6333.184068160001)\n",
            "DEBUG flwr 2023-12-14 06:58:17,798 | server.py:173 | evaluate_round 15: strategy sampled 4 clients (out of 4)\n",
            "DEBUG:flwr:evaluate_round 15: strategy sampled 4 clients (out of 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS: 0.7032623887062073\n",
            "BEST_LOSS: 0.7028810977935791\n",
            "ACCURACY: 0.5249999761581421\n",
            "BEST_ACCURACY: 0.5237500071525574\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR flwr 2023-12-14 06:58:51,401 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-12-14 06:58:51,408 | ray_client_proxy.py:146 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 6f952b7c23f80b39d4f218f0cab7c7acd4e9ded004d400b8d62c77e0) where the task (actor ID: fb26d7c08cc3e58a84f15e7201000000, name=DefaultActor.__init__, pid=10611, memory used=7.31GB) was running was 48.63GB / 50.99GB (0.953633), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-3adeda4d177f02f8ea52e3cadea29528c523c9e11d704f2adae75f8b*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "4975\t8.90\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0d932252-1132...\n",
            "10611\t7.31\tray::DefaultActor.run\n",
            "10607\t7.15\tray::DefaultActor.run\n",
            "10609\t5.28\tray::DefaultActor\n",
            "10610\t5.01\tray::DefaultActor\n",
            "5332\t0.41\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:a1a052b0830ea5d05...\n",
            "110\t0.09\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "5625\t0.08\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "10401\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "10342\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "DEBUG flwr 2023-12-14 06:59:06,237 | server.py:187 | evaluate_round 15 received 3 results and 1 failures\n",
            "DEBUG:flwr:evaluate_round 15 received 3 results and 1 failures\n",
            "INFO flwr 2023-12-14 06:59:06,239 | server.py:153 | FL finished in 6381.626889110999\n",
            "INFO:flwr:FL finished in 6381.626889110999\n",
            "INFO flwr 2023-12-14 06:59:06,242 | app.py:226 | app_fit: losses_distributed [(1, 0.7157793387974778), (2, 0.714014028849667), (3, 0.7127048846793501), (4, 0.7114577195415758), (5, 0.7128931681315104), (6, 0.7048258738084273), (7, 0.7043153090910478), (8, 0.7037878459150141), (9, 0.706842268596996), (10, 0.7077438632647196), (11, 0.7020710002292286), (12, 0.7087349328127774), (13, 0.7009935173121365), (14, 0.7078488978472623), (15, 0.6999499526890841)]\n",
            "INFO:flwr:app_fit: losses_distributed [(1, 0.7157793387974778), (2, 0.714014028849667), (3, 0.7127048846793501), (4, 0.7114577195415758), (5, 0.7128931681315104), (6, 0.7048258738084273), (7, 0.7043153090910478), (8, 0.7037878459150141), (9, 0.706842268596996), (10, 0.7077438632647196), (11, 0.7020710002292286), (12, 0.7087349328127774), (13, 0.7009935173121365), (14, 0.7078488978472623), (15, 0.6999499526890841)]\n",
            "INFO flwr 2023-12-14 06:59:06,245 | app.py:227 | app_fit: metrics_distributed_fit {}\n",
            "INFO:flwr:app_fit: metrics_distributed_fit {}\n",
            "INFO flwr 2023-12-14 06:59:06,249 | app.py:228 | app_fit: metrics_distributed {'accuracy': [(1, 0.5214041030570252), (2, 0.5218321836158021), (3, 0.5218321836158021), (4, 0.5222602641745789), (5, 0.5162037014961243), (6, 0.5238636255264282), (7, 0.5210227272727272), (8, 0.5215909069234674), (9, 0.5244318051771684), (10, 0.5179398059844971), (11, 0.5221590865742076), (12, 0.5227272857319225), (13, 0.5221591060811823), (14, 0.5215909069234674), (15, 0.5187500086697665)]}\n",
            "INFO:flwr:app_fit: metrics_distributed {'accuracy': [(1, 0.5214041030570252), (2, 0.5218321836158021), (3, 0.5218321836158021), (4, 0.5222602641745789), (5, 0.5162037014961243), (6, 0.5238636255264282), (7, 0.5210227272727272), (8, 0.5215909069234674), (9, 0.5244318051771684), (10, 0.5179398059844971), (11, 0.5221590865742076), (12, 0.5227272857319225), (13, 0.5221591060811823), (14, 0.5215909069234674), (15, 0.5187500086697665)]}\n",
            "INFO flwr 2023-12-14 06:59:06,251 | app.py:229 | app_fit: losses_centralized [(0, 0.7101017236709595), (1, 0.709667980670929), (2, 0.7094146013259888), (3, 0.7095325589179993), (4, 0.7093674540519714), (5, 0.7088690400123596), (6, 0.7085859179496765), (7, 0.7081944346427917), (8, 0.7079393863677979), (9, 0.7072866559028625), (10, 0.7064116597175598), (11, 0.7057126760482788), (12, 0.7047927379608154), (13, 0.7041099667549133), (14, 0.7028810977935791), (15, 0.7032623887062073)]\n",
            "INFO:flwr:app_fit: losses_centralized [(0, 0.7101017236709595), (1, 0.709667980670929), (2, 0.7094146013259888), (3, 0.7095325589179993), (4, 0.7093674540519714), (5, 0.7088690400123596), (6, 0.7085859179496765), (7, 0.7081944346427917), (8, 0.7079393863677979), (9, 0.7072866559028625), (10, 0.7064116597175598), (11, 0.7057126760482788), (12, 0.7047927379608154), (13, 0.7041099667549133), (14, 0.7028810977935791), (15, 0.7032623887062073)]\n",
            "INFO flwr 2023-12-14 06:59:06,254 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.5224999785423279), (1, 0.5212500095367432), (2, 0.5162500143051147), (3, 0.5162500143051147), (4, 0.5149999856948853), (5, 0.518750011920929), (6, 0.5149999856948853), (7, 0.5199999809265137), (8, 0.5212500095367432), (9, 0.518750011920929), (10, 0.5199999809265137), (11, 0.5237500071525574), (12, 0.5249999761581421), (13, 0.5249999761581421), (14, 0.5237500071525574), (15, 0.5249999761581421)]}\n",
            "INFO:flwr:app_fit: metrics_centralized {'accuracy': [(0, 0.5224999785423279), (1, 0.5212500095367432), (2, 0.5162500143051147), (3, 0.5162500143051147), (4, 0.5149999856948853), (5, 0.518750011920929), (6, 0.5149999856948853), (7, 0.5199999809265137), (8, 0.5212500095367432), (9, 0.518750011920929), (10, 0.5199999809265137), (11, 0.5237500071525574), (12, 0.5249999761581421), (13, 0.5249999761581421), (14, 0.5237500071525574), (15, 0.5249999761581421)]}\n"
          ]
        }
      ],
      "source": [
        "best_accuracy = 0.0\n",
        "best_loss = 999\n",
        "weights = np.array([])\n",
        "\n",
        "def evaluate(\n",
        "    server_round: int,\n",
        "    parameters: fl.common.NDArrays,\n",
        "    config: Dict[str, fl.common.Scalar],\n",
        "    ) -> Optional[Tuple[float, Dict[str, fl.common.Scalar]]]:\n",
        "    \"\"\"Centralized evaluation function\"\"\"\n",
        "\n",
        "    input_shape = (15001, 13)\n",
        "    model = get_model(input_shape)\n",
        "\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    model.compile(\"sgd\", loss=loss, metrics=[\"accuracy\"])\n",
        "    model.set_weights(parameters)\n",
        "\n",
        "    loss, accuracy = model.evaluate(val_samples, val_labels_df, batch_size=16, verbose=0)\n",
        "\n",
        "    global best_accuracy\n",
        "    global best_loss\n",
        "    global weights\n",
        "\n",
        "    print(f\"LOSS: {loss}\")\n",
        "    print(f\"BEST_LOSS: {best_loss}\")\n",
        "    print(f\"ACCURACY: {accuracy}\")\n",
        "    print(f\"BEST_ACCURACY: {best_accuracy}\")\n",
        "\n",
        "    if loss < best_loss:\n",
        "        best_accuracy = accuracy\n",
        "        weights = parameters\n",
        "        best_loss = loss\n",
        "\n",
        "    return loss, {\"accuracy\": accuracy}\n",
        "\n",
        "\n",
        "\n",
        "client_resources = {\"num_cpus\": 2}\n",
        "if tf.config.get_visible_devices(\"GPU\"):\n",
        "    client_resources[\"num_gpus\"] = 1\n",
        "\n",
        "# Specify the Strategy\n",
        "strategy = fl.server.strategy.FedAvg(\n",
        "    fraction_fit=1.0,  # Sample 100% of available clients for training\n",
        "    fraction_evaluate=1.0,  \n",
        "    min_fit_clients=NUM_CLIENTS,  \n",
        "    min_evaluate_clients=NUM_CLIENTS,  \n",
        "    min_available_clients=NUM_CLIENTS,  # Wait until all 4 clients are available\n",
        "    evaluate_metrics_aggregation_fn=weighted_average,\n",
        "    evaluate_fn=evaluate\n",
        ")\n",
        "\n",
        "# Start simulation\n",
        "history = fl.simulation.start_simulation(\n",
        "    client_fn=create_client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=15),\n",
        "    strategy=strategy,\n",
        "    client_resources=client_resources,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO-YYWSD7NcS",
        "outputId": "82041114-305b-4b56-f93a-66efcbf4fd1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "History (loss, distributed):\n",
              "\tround 1: 0.7157793387974778\n",
              "\tround 2: 0.714014028849667\n",
              "\tround 3: 0.7127048846793501\n",
              "\tround 4: 0.7114577195415758\n",
              "\tround 5: 0.7128931681315104\n",
              "\tround 6: 0.7048258738084273\n",
              "\tround 7: 0.7043153090910478\n",
              "\tround 8: 0.7037878459150141\n",
              "\tround 9: 0.706842268596996\n",
              "\tround 10: 0.7077438632647196\n",
              "\tround 11: 0.7020710002292286\n",
              "\tround 12: 0.7087349328127774\n",
              "\tround 13: 0.7009935173121365\n",
              "\tround 14: 0.7078488978472623\n",
              "\tround 15: 0.6999499526890841\n",
              "History (loss, centralized):\n",
              "\tround 0: 0.7101017236709595\n",
              "\tround 1: 0.709667980670929\n",
              "\tround 2: 0.7094146013259888\n",
              "\tround 3: 0.7095325589179993\n",
              "\tround 4: 0.7093674540519714\n",
              "\tround 5: 0.7088690400123596\n",
              "\tround 6: 0.7085859179496765\n",
              "\tround 7: 0.7081944346427917\n",
              "\tround 8: 0.7079393863677979\n",
              "\tround 9: 0.7072866559028625\n",
              "\tround 10: 0.7064116597175598\n",
              "\tround 11: 0.7057126760482788\n",
              "\tround 12: 0.7047927379608154\n",
              "\tround 13: 0.7041099667549133\n",
              "\tround 14: 0.7028810977935791\n",
              "\tround 15: 0.7032623887062073\n",
              "History (metrics, distributed, evaluate):\n",
              "{'accuracy': [(1, 0.5214041030570252), (2, 0.5218321836158021), (3, 0.5218321836158021), (4, 0.5222602641745789), (5, 0.5162037014961243), (6, 0.5238636255264282), (7, 0.5210227272727272), (8, 0.5215909069234674), (9, 0.5244318051771684), (10, 0.5179398059844971), (11, 0.5221590865742076), (12, 0.5227272857319225), (13, 0.5221591060811823), (14, 0.5215909069234674), (15, 0.5187500086697665)]}History (metrics, centralized):\n",
              "{'accuracy': [(0, 0.5224999785423279), (1, 0.5212500095367432), (2, 0.5162500143051147), (3, 0.5162500143051147), (4, 0.5149999856948853), (5, 0.518750011920929), (6, 0.5149999856948853), (7, 0.5199999809265137), (8, 0.5212500095367432), (9, 0.518750011920929), (10, 0.5199999809265137), (11, 0.5237500071525574), (12, 0.5249999761581421), (13, 0.5249999761581421), (14, 0.5237500071525574), (15, 0.5249999761581421)]}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-29DaJU7PWE",
        "outputId": "21409db2-e311-4c93-d206-429f7f735fcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5237500071525574\n",
            "0.7028810977935791\n",
            "[array([[ 0.13315667, -0.08365704,  0.02030483, ..., -0.10064241,\n",
            "         0.14401144,  0.16280267],\n",
            "       [ 0.12883452,  0.04889979, -0.11882403, ...,  0.05063355,\n",
            "         0.03609406, -0.13403752],\n",
            "       [-0.05688292, -0.16078037, -0.15416409, ..., -0.16388075,\n",
            "        -0.1185234 , -0.03006899],\n",
            "       ...,\n",
            "       [-0.08200052,  0.01914796,  0.09409245, ...,  0.1396507 ,\n",
            "        -0.11295156,  0.14699434],\n",
            "       [-0.161084  , -0.16690877,  0.12437475, ..., -0.14452769,\n",
            "        -0.00854014, -0.15505445],\n",
            "       [-0.16622221,  0.13332364,  0.15814263, ...,  0.14752242,\n",
            "        -0.00777462, -0.02621231]], dtype=float32), array([[-0.02025299, -0.15541996, -0.06808344, ..., -0.04007347,\n",
            "        -0.09611283,  0.01753339],\n",
            "       [ 0.04909715, -0.08766202, -0.04154299, ..., -0.16929613,\n",
            "         0.05666951,  0.00407448],\n",
            "       [ 0.05029881, -0.11540747,  0.08810554, ..., -0.13017213,\n",
            "         0.08423945,  0.0547911 ],\n",
            "       ...,\n",
            "       [-0.05467908, -0.05476128,  0.07586455, ...,  0.10234635,\n",
            "        -0.05841666,  0.00626041],\n",
            "       [-0.07000315,  0.08994798, -0.08642215, ..., -0.0597898 ,\n",
            "         0.0409442 , -0.00936206],\n",
            "       [-0.09345206, -0.08253824,  0.03609891, ..., -0.03969543,\n",
            "         0.00438645,  0.08869714]], dtype=float32), array([[-4.41281183e-04,  6.63312094e-04,  7.10108026e-04,\n",
            "        -7.09837841e-05, -5.06762954e-05, -6.83961203e-04,\n",
            "         4.29460255e-04, -9.25752029e-05, -1.73365828e-04,\n",
            "         3.80746060e-04,  2.94461352e-05,  2.53189297e-04,\n",
            "         4.82451112e-04,  5.39602304e-04, -4.23002435e-04,\n",
            "        -9.24176857e-05, -2.40388021e-04,  1.45243411e-03,\n",
            "         2.45760806e-04, -1.07085996e-03,  2.92233046e-04,\n",
            "        -5.43145397e-05,  6.48163899e-04,  6.50804257e-04,\n",
            "         3.75649019e-04,  1.20577600e-03, -2.48414726e-04,\n",
            "         3.44645960e-04,  8.13212304e-04,  7.45847297e-04,\n",
            "         5.95138627e-05,  9.83581063e-04, -1.38194091e-03,\n",
            "         1.31168612e-03, -6.21254294e-05, -1.00068678e-03,\n",
            "        -9.30546899e-04, -8.80322536e-04,  6.32109761e-04,\n",
            "        -2.14530621e-04,  2.30932215e-04,  3.12575517e-04,\n",
            "         9.31312206e-06,  6.35179109e-04, -6.23200976e-05,\n",
            "         3.11619588e-05, -2.28146862e-04, -6.52213639e-04,\n",
            "         9.16052726e-04, -3.10699601e-04, -1.34661316e-03,\n",
            "        -1.03184569e-03, -2.74559599e-04, -8.55605525e-04,\n",
            "         6.22087566e-04,  6.22896710e-04, -7.42634118e-04,\n",
            "        -3.45800101e-04, -5.48636366e-04,  4.23727353e-04,\n",
            "         5.91053569e-04, -2.73613579e-04,  5.34072577e-04,\n",
            "        -9.73037386e-05,  9.46759479e-04,  3.73957999e-04,\n",
            "         3.42007785e-04,  2.07273770e-04, -8.14343628e-04,\n",
            "         8.42979120e-04,  6.13794255e-04, -1.60875104e-04,\n",
            "        -1.53049754e-04, -4.70177853e-04, -1.48204403e-04,\n",
            "        -1.01289875e-03, -3.24709690e-04,  2.19681053e-04,\n",
            "         1.11870945e-03,  6.84422732e-04, -5.32770500e-05,\n",
            "         4.52927343e-04, -1.09130540e-03,  1.54394569e-04,\n",
            "        -1.23840582e-03,  5.83842513e-04,  3.47611698e-04,\n",
            "        -6.74662006e-04,  3.90136411e-04,  6.60016143e-04,\n",
            "         6.02653483e-04,  2.13895721e-04,  2.48873257e-04,\n",
            "        -2.59000459e-04,  8.07541248e-04, -1.02148438e-03,\n",
            "         5.21330803e-04,  4.20022610e-04, -4.96357272e-04,\n",
            "        -1.46615063e-03, -2.71734432e-04, -1.52613811e-05,\n",
            "         3.06491420e-04,  1.36745640e-03,  8.55789636e-04,\n",
            "        -6.49878988e-04, -1.05469534e-03,  4.56155714e-04,\n",
            "         6.22900960e-04, -1.62403026e-04,  3.60059174e-04,\n",
            "         3.19482875e-04, -6.45431515e-04,  1.08816188e-04,\n",
            "        -4.60005744e-04, -6.19899249e-04, -4.42338060e-04,\n",
            "         3.05725116e-04, -4.61453892e-04, -5.20985981e-04,\n",
            "        -7.23602658e-04, -7.29790365e-04, -5.06551733e-05,\n",
            "        -1.13997469e-03, -1.55831498e-04, -3.16393940e-04,\n",
            "        -5.70492703e-04, -5.41569025e-04,  1.11586915e-03,\n",
            "         3.42894869e-04,  4.50544641e-04, -8.84580368e-04,\n",
            "         1.30424902e-04,  8.40011984e-04, -8.30706558e-04,\n",
            "        -3.18157632e-04,  2.80467910e-04,  6.82188896e-04,\n",
            "        -2.45091542e-05, -2.56881147e-04, -1.91372004e-04,\n",
            "         1.00984622e-03,  4.14980779e-04, -5.48171112e-04,\n",
            "         7.27503735e-04,  3.78619385e-04, -3.12317570e-04,\n",
            "        -1.49997752e-04,  1.89007857e-04, -2.97872321e-05,\n",
            "        -5.50790166e-04,  1.24298036e-03,  9.41440172e-04,\n",
            "        -6.27937028e-04, -1.04744558e-03,  1.07219524e-03,\n",
            "        -4.41412471e-04, -8.25122406e-04,  3.38282465e-04,\n",
            "        -4.52985289e-04, -4.70735074e-04, -1.40713004e-03,\n",
            "         9.24786029e-04,  8.23916402e-04,  5.72201097e-04,\n",
            "        -2.16459273e-03, -1.32583745e-03, -2.91622855e-04,\n",
            "        -1.49977754e-03,  2.16316257e-04,  1.35821188e-04,\n",
            "        -2.72704725e-04,  3.91790294e-04,  4.02060105e-04,\n",
            "        -5.28899429e-04, -2.17796638e-04,  5.00164286e-04,\n",
            "        -3.51316528e-04,  6.84578961e-04, -9.41588078e-05,\n",
            "        -4.10293113e-04, -2.77975458e-04, -3.37906997e-04,\n",
            "         2.36290754e-04,  4.83001961e-04,  2.47003016e-04,\n",
            "        -6.10828865e-05,  5.80056330e-05, -9.28071240e-05,\n",
            "         2.09097401e-04,  5.71054581e-04,  1.12239132e-03],\n",
            "       [ 1.85576675e-04, -4.99552523e-04, -1.00817299e-03,\n",
            "        -9.69472865e-04,  2.91846023e-04, -8.77677172e-04,\n",
            "        -1.76574642e-04,  2.01207396e-04,  1.25022008e-04,\n",
            "        -2.61662877e-04,  8.65758164e-04,  6.28000649e-04,\n",
            "        -1.53357818e-04,  7.31483800e-04, -2.12702769e-04,\n",
            "         6.48003188e-04, -4.54853958e-04, -1.28923880e-03,\n",
            "         8.98201542e-04,  2.28794001e-04,  8.45656847e-04,\n",
            "         4.96211054e-04, -1.31603109e-03, -7.40113726e-04,\n",
            "         2.79014930e-04,  9.94215297e-05,  9.19651764e-04,\n",
            "        -1.28410888e-04,  2.50033132e-04,  3.35920166e-04,\n",
            "        -3.07693525e-04, -4.14704147e-04, -5.85846137e-04,\n",
            "         1.28626765e-03,  7.92066858e-05, -1.87248574e-04,\n",
            "        -6.72878581e-04, -1.60974247e-04,  9.62233113e-04,\n",
            "         4.75271227e-04,  7.17555231e-04,  8.21380585e-04,\n",
            "         9.07373033e-05,  1.81393101e-04,  9.92423389e-04,\n",
            "         8.03035917e-04,  5.38243446e-04, -3.89429537e-04,\n",
            "        -2.73429585e-04,  5.17679982e-05, -1.56228628e-03,\n",
            "         1.26264749e-05,  1.03468228e-04, -6.09119190e-04,\n",
            "         4.17002244e-04, -5.52469923e-04,  2.56112777e-04,\n",
            "         2.20691232e-04, -1.16477546e-04,  7.07117608e-04,\n",
            "        -2.72238482e-04, -4.59370785e-04,  1.08624354e-03,\n",
            "         2.82755384e-04,  4.05538507e-04, -3.80271158e-05,\n",
            "         4.50203224e-04, -2.49683973e-04, -4.10695706e-04,\n",
            "        -1.05826720e-03, -2.67292809e-04, -8.21637455e-04,\n",
            "         8.59184714e-04, -6.33660180e-04, -1.51004075e-04,\n",
            "         5.50484110e-04, -4.82864882e-04,  8.12982791e-04,\n",
            "         1.91387298e-05, -4.07004380e-04,  2.16804352e-03,\n",
            "        -3.79151112e-04, -5.21034926e-05,  3.13264463e-04,\n",
            "        -1.94597407e-04, -9.23918415e-05,  8.87411821e-04,\n",
            "         4.31798893e-04, -5.56382292e-04, -1.26192070e-04,\n",
            "         1.17570715e-04,  1.78845064e-03,  5.10910526e-04,\n",
            "         3.81354446e-04, -8.02506169e-04, -4.31658933e-04,\n",
            "        -1.22100476e-03, -8.60687112e-04,  4.32279863e-04,\n",
            "         5.62650603e-05,  5.35709376e-04, -5.46142575e-04,\n",
            "        -5.77354920e-04, -7.11468892e-05,  6.25291839e-04,\n",
            "        -6.43182138e-04,  4.10872628e-04,  4.61051619e-04,\n",
            "         4.36663133e-04,  2.14694097e-04,  3.50466958e-04,\n",
            "         8.07362521e-05, -4.98357927e-04, -4.54407942e-04,\n",
            "         2.74760114e-05, -3.58732301e-04,  2.84745474e-05,\n",
            "        -8.35996525e-06,  3.91367284e-05, -9.99947777e-04,\n",
            "         4.77025635e-04,  7.84061383e-04,  2.92534241e-04,\n",
            "         3.04666290e-04, -9.09930852e-04,  2.35853280e-04,\n",
            "        -3.46936984e-04, -1.55173213e-04,  3.55831762e-05,\n",
            "        -4.04320308e-04,  3.30507813e-04,  4.41752636e-04,\n",
            "        -9.54846560e-04,  6.65386382e-04, -7.77102541e-05,\n",
            "        -6.72522059e-04,  2.61536043e-04,  5.03746909e-04,\n",
            "        -2.97490740e-04, -5.34231658e-04, -4.72430576e-04,\n",
            "         3.33079108e-04,  7.58224982e-04, -3.81647958e-04,\n",
            "         8.23443290e-04,  2.58944550e-04,  2.97883322e-04,\n",
            "         9.74866329e-04,  3.16405261e-04,  4.58700611e-04,\n",
            "         5.36030311e-05, -7.68540369e-04, -7.01579906e-04,\n",
            "        -1.43461686e-04, -4.42465913e-04, -1.25287054e-03,\n",
            "         1.17674866e-03,  4.62303084e-04, -3.57656012e-04,\n",
            "        -1.13508629e-03, -5.35686559e-04, -9.90708941e-04,\n",
            "        -2.67363994e-05,  5.64115180e-04,  1.86144563e-04,\n",
            "        -6.21534346e-05, -8.25544157e-06, -1.18222873e-04,\n",
            "         3.76665354e-04, -1.28551736e-03,  3.11512151e-04,\n",
            "         3.46996880e-04, -3.21976899e-04, -3.94401373e-04,\n",
            "        -6.09654177e-04,  3.99179145e-04,  4.78165457e-04,\n",
            "        -1.09734514e-03,  2.85951101e-04, -8.36334890e-04,\n",
            "         2.19391019e-04,  4.20911441e-04,  8.79779051e-04,\n",
            "        -1.09644279e-04,  8.03668401e-04,  6.60860562e-04,\n",
            "        -1.09589531e-03,  7.33545676e-05, -8.79934523e-04,\n",
            "         9.19205369e-04, -5.34527877e-04,  3.42782820e-04]], dtype=float32), array([[ 0.19641858,  0.21431962,  0.08453324, ..., -0.20287842,\n",
            "        -0.01312521, -0.03768447],\n",
            "       [-0.18478046, -0.01726319, -0.15436022, ..., -0.14389731,\n",
            "         0.03475173, -0.05178228],\n",
            "       [ 0.12566572, -0.06614661,  0.22222091, ..., -0.01766525,\n",
            "        -0.0812299 ,  0.22264929],\n",
            "       ...,\n",
            "       [-0.07250161, -0.14719777,  0.00822795, ..., -0.03391284,\n",
            "         0.04235041, -0.21631958],\n",
            "       [ 0.0421319 , -0.09491788, -0.07432589, ..., -0.24374124,\n",
            "        -0.02473346,  0.00672647],\n",
            "       [-0.21335948, -0.19630641,  0.14320719, ...,  0.00380712,\n",
            "         0.07918989,  0.18823938]], dtype=float32), array([-4.4162862e-04, -2.2368314e-04, -4.3945780e-04, -2.3686532e-04,\n",
            "       -1.6785321e-05,  3.0236624e-04, -1.1912672e-03, -9.8513614e-04,\n",
            "        1.1137436e-03, -1.4072859e-04,  2.1937328e-04, -7.4320432e-04,\n",
            "       -1.3143038e-03,  2.1398293e-04,  5.0631956e-05, -5.8190191e-05,\n",
            "        5.5840972e-04, -2.4667248e-04,  1.1646115e-03,  7.7115180e-04,\n",
            "        4.1341063e-04, -1.4802408e-03, -1.9003528e-04, -3.8478908e-04,\n",
            "        7.3483825e-04,  5.1242026e-04,  9.3164817e-05, -1.7774322e-04,\n",
            "        4.4070353e-04, -1.4512802e-03,  2.0197892e-04, -1.3482705e-04],\n",
            "      dtype=float32), array([[-0.09550013],\n",
            "       [-0.19576673],\n",
            "       [-0.29612726],\n",
            "       [-0.40164816],\n",
            "       [ 0.10218539],\n",
            "       [ 0.22798389],\n",
            "       [ 0.06407505],\n",
            "       [-0.24925062],\n",
            "       [-0.07759418],\n",
            "       [-0.18962687],\n",
            "       [ 0.30342942],\n",
            "       [ 0.3344257 ],\n",
            "       [ 0.1864748 ],\n",
            "       [ 0.10710043],\n",
            "       [-0.40554753],\n",
            "       [ 0.38830638],\n",
            "       [ 0.02396782],\n",
            "       [-0.15308282],\n",
            "       [ 0.36588752],\n",
            "       [ 0.38067982],\n",
            "       [ 0.01438536],\n",
            "       [ 0.38287777],\n",
            "       [-0.40902007],\n",
            "       [-0.39290974],\n",
            "       [-0.28388536],\n",
            "       [ 0.32811922],\n",
            "       [-0.09968089],\n",
            "       [ 0.38926628],\n",
            "       [ 0.2836649 ],\n",
            "       [ 0.21122043],\n",
            "       [ 0.40315768],\n",
            "       [ 0.1457124 ]], dtype=float32), array([0.00128641], dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "# printing the validation results\n",
        "print(best_accuracy)\n",
        "print(best_loss)\n",
        "print(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L4NkzZuKvHB",
        "outputId": "5b255343-c6a6-47c8-8927-4051a4c2dba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] evaluating on testing set...\n",
            "70/70 [==============================] - 50s 706ms/step - loss: 0.6572 - accuracy: 0.6482\n",
            "[INFO] loss=0.6572, accuracy: 64.8214%\n"
          ]
        }
      ],
      "source": [
        "# test results\n",
        "input_shape = (15001, 13)\n",
        "test_model = get_model(input_shape)\n",
        "test_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1), metrics=['accuracy'])\n",
        "\n",
        "test_model.set_weights(weights)\n",
        "\n",
        "print(\"[INFO] evaluating on testing set...\")\n",
        "(test_loss, test_accuracy) = test_model.evaluate(test_samples, test_labels, batch_size=8, verbose=1)\n",
        "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(test_loss, test_accuracy * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF1ptwc1L8Se",
        "outputId": "069f26e3-6735-414b-d450-90a26714d234"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 273/273 [01:25<00:00,  3.21it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "test_preds = []\n",
        "\n",
        "file_ids = os.listdir(data_dir)\n",
        "test_label_file = pd.read_csv(\"edaicwoz/test_split.csv\")\n",
        "\n",
        "for file_id in tqdm(file_ids):\n",
        "    if int(file_id) in test_label_file[\"Participant_ID\"].values:\n",
        "        all_mfccs = []\n",
        "        for j in range(10):\n",
        "            file_path = \"MFCCs_1030/\" + file_id + \"/\" + file_id + \"_MFCC_\" + str(j) + \".npy\"\n",
        "            all_mfccs.append(np.load(file_path))\n",
        "        all_mfccs = np.array(all_mfccs)\n",
        "        all_mfccs = np.swapaxes(all_mfccs, 1, 2)\n",
        "        prediction = test_model.predict(all_mfccs, verbose=0, batch_size=8)\n",
        "        prediction = prediction.mean(axis=0)\n",
        "        test_preds.append(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "Cnj3Z4aUL1vo",
        "outputId": "7034b376-3501-4fb5-d6d7-98564d9c760d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.97      0.81        39\n",
            "           1       0.00      0.00      0.00        17\n",
            "\n",
            "    accuracy                           0.68        56\n",
            "   macro avg       0.35      0.49      0.40        56\n",
            "weighted avg       0.48      0.68      0.56        56\n",
            "\n"
          ]
        }
      ],
      "source": [
        "label_file = test_label_file\n",
        "true = label_file.to_numpy()[:, 2].flatten().tolist()\n",
        "predicted = (np.array(test_preds) > 0.5).astype(np.int32).flatten().tolist()\n",
        "\n",
        "\n",
        "\n",
        "true_labels = true\n",
        "predicted_labels = predicted\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_confusion_matrix(true_labels, predicted_labels):\n",
        "    true_labels = np.array(true_labels)\n",
        "    predicted_labels = np.array(predicted_labels)\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "    return cm\n",
        "\n",
        "def plot_confusion_matrix(conf_matrix, classes):\n",
        "    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    for i in range(len(classes)):\n",
        "        for j in range(len(classes)):\n",
        "            plt.text(j, i, str(conf_matrix[i, j]), ha='center', va='center', color='white' if conf_matrix[i, j] > conf_matrix.max() / 2 else 'black')\n",
        "\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()\n",
        "\n",
        "def calculate_metrics(true_labels, predicted_labels):\n",
        "    precision = precision_score(true_labels, predicted_labels)\n",
        "    recall = recall_score(true_labels, predicted_labels)\n",
        "    f1 = f1_score(true_labels, predicted_labels)\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "conf_matrix = create_confusion_matrix(true_labels, predicted_labels)\n",
        "classes = ['0', '1']\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(conf_matrix, classes)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "\n",
        "# Calculate precision, recall, and F1-score for both classes\n",
        "classification_report = classification_report(true_labels, predicted_labels, target_names=classes)\n",
        "\n",
        "print(classification_report)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
